{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad87999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifeishi/Desktop/NEU/fall 2025/CS 6120/Project/nlp_project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict, Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387ea699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yifeishi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yifeishi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('trial.n.02'),\n",
       " Synset('test.n.02'),\n",
       " Synset('examination.n.02'),\n",
       " Synset('test.n.04'),\n",
       " Synset('test.n.05'),\n",
       " Synset('test.n.06'),\n",
       " Synset('test.v.01'),\n",
       " Synset('screen.v.01'),\n",
       " Synset('quiz.v.01'),\n",
       " Synset('test.v.04'),\n",
       " Synset('test.v.05'),\n",
       " Synset('test.v.06'),\n",
       " Synset('test.v.07')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data files\n",
    "# Download WordNet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wn.synsets('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfb9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ad755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (103063, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Price Starting With ($)</th>\n",
       "      <th>Publish Date (Month)</th>\n",
       "      <th>Publish Date (Year)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goat Brothers</td>\n",
       "      <td>By Colton, Larry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>History , General</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>8.79</td>\n",
       "      <td>January</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Missing Person</td>\n",
       "      <td>By Grumbach, Doris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fiction , General</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>4.99</td>\n",
       "      <td>March</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don't Eat Your Heart Out Cookbook</td>\n",
       "      <td>By Piscatella, Joseph C.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooking , Reference</td>\n",
       "      <td>Workman Pub Co</td>\n",
       "      <td>4.99</td>\n",
       "      <td>September</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Your Corporate Umbrella Begins to Leak: A...</td>\n",
       "      <td>By Davis, Paul D.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natl Pr Books</td>\n",
       "      <td>4.99</td>\n",
       "      <td>April</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy Spangler's Breastfeeding : A Parent's Guide</td>\n",
       "      <td>By Spangler, Amy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amy Spangler</td>\n",
       "      <td>5.32</td>\n",
       "      <td>February</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                                      Goat Brothers   \n",
       "1                                 The Missing Person   \n",
       "2                  Don't Eat Your Heart Out Cookbook   \n",
       "3  When Your Corporate Umbrella Begins to Leak: A...   \n",
       "4    Amy Spangler's Breastfeeding : A Parent's Guide   \n",
       "\n",
       "                    Authors Description              Category  \\\n",
       "0          By Colton, Larry         NaN     History , General   \n",
       "1        By Grumbach, Doris         NaN     Fiction , General   \n",
       "2  By Piscatella, Joseph C.         NaN   Cooking , Reference   \n",
       "3         By Davis, Paul D.         NaN                   NaN   \n",
       "4          By Spangler, Amy         NaN                   NaN   \n",
       "\n",
       "          Publisher  Price Starting With ($) Publish Date (Month)  \\\n",
       "0         Doubleday                     8.79              January   \n",
       "1  Putnam Pub Group                     4.99                March   \n",
       "2    Workman Pub Co                     4.99            September   \n",
       "3     Natl Pr Books                     4.99                April   \n",
       "4      Amy Spangler                     5.32             February   \n",
       "\n",
       "   Publish Date (Year)  \n",
       "0                 1993  \n",
       "1                 1981  \n",
       "2                 1983  \n",
       "3                 1991  \n",
       "4                 1997  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"BooksDatasetClean.csv\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e46a73",
   "metadata": {},
   "source": [
    "# Label Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d98f1",
   "metadata": {},
   "source": [
    "## Category Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b699dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing missing categories/descriptions: (65296, 8)\n"
     ]
    }
   ],
   "source": [
    "# Eliminate rows with missing category & description\n",
    "df = df.dropna(subset=['Category', 'Description'])\n",
    "print(f\"After removing missing categories/descriptions: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c8bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add book_id for tracking\n",
    "df['book_id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315c827a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2026"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new list, then put each unique category (separated by comma) into the list\n",
    "category_list = []\n",
    "for categories in df['Category']:\n",
    "    for category in categories.split(','):\n",
    "        category = category.strip()\n",
    "        if category not in category_list:\n",
    "            category_list.append(category)\n",
    "\n",
    "len(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48e702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('General', 25721),\n",
       " ('Fiction', 17721),\n",
       " ('Juvenile Fiction', 6437),\n",
       " ('Religion', 3686),\n",
       " ('Romance', 3491),\n",
       " ('Cooking', 2890),\n",
       " ('Juvenile Nonfiction', 2665),\n",
       " ('History', 2583),\n",
       " ('Business & Economics', 2542),\n",
       " ('Historical', 2356)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top 10 frequent categories in the list\n",
    "category_counter = Counter()\n",
    "for categories in df['Category']:\n",
    "    for category in categories.split(','):\n",
    "        category = category.strip()\n",
    "        category_counter[category] += 1\n",
    "category_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978676a0",
   "metadata": {},
   "source": [
    "Category 'General' appears for almost 40% of the valid records but provides no discriminative information for classification, need to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e205f8",
   "metadata": {},
   "source": [
    "Other than removing noise, we need to preprocess categories in the dataset before clustering, which includes:\n",
    "    - Remove 'General' category\n",
    "    - Remove parentheses and their contents\n",
    "    - Remove numbers\n",
    "    - Normalize punctuation\n",
    "    - Lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90614daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_category(cat):\n",
    "    if pd.isna(cat): return None\n",
    "    cat = str(cat).strip()\n",
    "    \n",
    "    # FIRST: Replace & with , to treat it as separator\n",
    "    cat = cat.replace(' & ', ', ')\n",
    "    \n",
    "    # THEN: Split on comma\n",
    "    categories = [c.strip() for c in cat.split(',')]\n",
    "    \n",
    "    cleaned = []\n",
    "    for c in categories:\n",
    "        if c.lower().strip() == 'general': \n",
    "            continue\n",
    "        c = re.sub(r'\\(.*?\\)', '', c)\n",
    "        c = re.sub(r'[()]|\\d+', '', c)\n",
    "        c = c.replace(\"'s\", \"s\").replace(\"-\", \" \")  # Remove & replacement here\n",
    "        c = c.lower().strip()\n",
    "        c = ' '.join(c.split())\n",
    "        if c: \n",
    "            cleaned.append(c)\n",
    "    \n",
    "    return ', '.join(cleaned) if cleaned else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bde3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df['Category_Cleaned'] = df['Category'].apply(preprocess_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0abbb130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning (removed 'General' and invalid categories): (65296, 10)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where all categories were filtered out\n",
    "df = df[df['Category_Cleaned'].notna()]\n",
    "print(f\"After cleaning (removed 'General' and invalid categories): {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "046933b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of original vs cleaned categories:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Category_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Poetry , General</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Biography &amp; Autobiography , General</td>\n",
       "      <td>biography, autobiography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Health &amp; Fitness , Diet &amp; Nutrition , Diets</td>\n",
       "      <td>health, fitness, diet, nutrition, diets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Technology &amp; Engineering , Military Science</td>\n",
       "      <td>technology, engineering, military science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Religion , Biblical Biography , General</td>\n",
       "      <td>religion, biblical biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Biography &amp; Autobiography , Personal Memoirs</td>\n",
       "      <td>biography, autobiography, personal memoirs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Political Science , General</td>\n",
       "      <td>political science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pets , Cats , General</td>\n",
       "      <td>pets, cats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Political Science , General</td>\n",
       "      <td>political science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fiction , General</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Category  \\\n",
       "7                                Poetry , General   \n",
       "8             Biography & Autobiography , General   \n",
       "10    Health & Fitness , Diet & Nutrition , Diets   \n",
       "11    Technology & Engineering , Military Science   \n",
       "13        Religion , Biblical Biography , General   \n",
       "14   Biography & Autobiography , Personal Memoirs   \n",
       "19                    Political Science , General   \n",
       "21                          Pets , Cats , General   \n",
       "26                    Political Science , General   \n",
       "27                              Fiction , General   \n",
       "\n",
       "                              Category_Cleaned  \n",
       "7                                       poetry  \n",
       "8                     biography, autobiography  \n",
       "10     health, fitness, diet, nutrition, diets  \n",
       "11   technology, engineering, military science  \n",
       "13                religion, biblical biography  \n",
       "14  biography, autobiography, personal memoirs  \n",
       "19                           political science  \n",
       "21                                  pets, cats  \n",
       "26                           political science  \n",
       "27                                     fiction  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample of cleaned categories\n",
    "print(\"\\nSample of original vs cleaned categories:\")\n",
    "df[['Category', 'Category_Cleaned']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d21fc3",
   "metadata": {},
   "source": [
    "Noticed different taxnomies exists in the category (i.e. fiction is a type of literature, but religion is the theme of the book), we need to discuss whether further rul-based clustering needs to be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c36c3",
   "metadata": {},
   "source": [
    "## Word Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734dd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode categories - create one row per category per book\n",
    "df_exploded = df.copy()\n",
    "df_exploded['Category_Split'] = df_exploded['Category_Cleaned'].str.split(', ')\n",
    "df_exploded = df_exploded.explode('Category_Split')\n",
    "df_exploded = df_exploded[df_exploded['Category_Split'].notna()]\n",
    "df_exploded = df_exploded.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "435dddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After exploding categories: (170599, 11)\n",
      "Unique categories: 2131\n"
     ]
    }
   ],
   "source": [
    "print(f\"After exploding categories: {df_exploded.shape}\")\n",
    "print(f\"Unique categories: {df_exploded['Category_Split'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5632701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category_Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Collects poems written by the eleven-year-old ...</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>The Russian author offers an affectionate chro...</td>\n",
       "      <td>biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>The Russian author offers an affectionate chro...</td>\n",
       "      <td>autobiography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>A humor classic, this tongue-in-cheek diet pla...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>A humor classic, this tongue-in-cheek diet pla...</td>\n",
       "      <td>fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>A humor classic, this tongue-in-cheek diet pla...</td>\n",
       "      <td>diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>A humor classic, this tongue-in-cheek diet pla...</td>\n",
       "      <td>nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>A humor classic, this tongue-in-cheek diet pla...</td>\n",
       "      <td>diets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>Deadly germs sprayed in shopping malls, bomb-l...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>Deadly germs sprayed in shopping malls, bomb-l...</td>\n",
       "      <td>engineering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id                                        Description Category_Split\n",
       "0        7  Collects poems written by the eleven-year-old ...         poetry\n",
       "1        8  The Russian author offers an affectionate chro...      biography\n",
       "2        8  The Russian author offers an affectionate chro...  autobiography\n",
       "3       10  A humor classic, this tongue-in-cheek diet pla...         health\n",
       "4       10  A humor classic, this tongue-in-cheek diet pla...        fitness\n",
       "5       10  A humor classic, this tongue-in-cheek diet pla...           diet\n",
       "6       10  A humor classic, this tongue-in-cheek diet pla...      nutrition\n",
       "7       10  A humor classic, this tongue-in-cheek diet pla...          diets\n",
       "8       11  Deadly germs sprayed in shopping malls, bomb-l...     technology\n",
       "9       11  Deadly germs sprayed in shopping malls, bomb-l...    engineering"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded[['book_id', 'Description', 'Category_Split']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9d4bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize each category\n",
    "def lemmatize_category(cat):\n",
    "    \"\"\"\n",
    "    Lemmatize category and use lemmatized form directly as canonical.\n",
    "    For multi-word categories, lemmatize each word and reconstruct.\n",
    "    \"\"\"\n",
    "    if pd.isna(cat) or not cat:\n",
    "        return None\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(cat)\n",
    "    \n",
    "    # Lemmatize each token (excluding punctuation)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    \n",
    "    # Reconstruct with spaces\n",
    "    canonical = \" \".join(lemmas).strip()\n",
    "    \n",
    "    return canonical if canonical else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "195bc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded['Category_Lemmatized'] = df_exploded['Category_Split'].apply(lemmatize_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd38b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any None values\n",
    "df_exploded = df_exploded[df_exploded['Category_Lemmatized'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35c7bcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Category_Split Category_Lemmatized\n",
      "0               poetry              poetry\n",
      "1            biography           biography\n",
      "2        autobiography       autobiography\n",
      "3               health              health\n",
      "4              fitness             fitness\n",
      "5                 diet                diet\n",
      "6            nutrition           nutrition\n",
      "7                diets                diet\n",
      "8           technology          technology\n",
      "9          engineering         engineering\n",
      "10    military science    military science\n",
      "11            religion            religion\n",
      "12  biblical biography  biblical biography\n",
      "15    personal memoirs     personal memoir\n",
      "16   political science   political science\n",
      "17                pets                 pet\n",
      "18                cats                 cat\n",
      "20             fiction             fiction\n",
      "21             history             history\n",
      "22            military            military\n"
     ]
    }
   ],
   "source": [
    "sample_lemma = df_exploded[['Category_Split', 'Category_Lemmatized']].drop_duplicates().head(20)\n",
    "print(sample_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dbb913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique categories after lemmatization: 2083\n",
      "total book-category pairs: 170599\n"
     ]
    }
   ],
   "source": [
    "print(f\"unique categories after lemmatization: {df_exploded['Category_Lemmatized'].nunique()}\")\n",
    "print(f\"total book-category pairs: {len(df_exploded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a067a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cases where multiple original forms map to same lemma\n",
    "lemma_groups = df_exploded.groupby('Category_Lemmatized')['Category_Split'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfd9ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some interesting merges\n",
    "merge_examples = []\n",
    "for lemma, originals in lemma_groups.items():\n",
    "    if len(originals) > 1:\n",
    "        merge_examples.append({\n",
    "            'lemmatized': lemma,\n",
    "            'originals': list(originals),\n",
    "            'count': len(originals)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcd6cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by number of variants merged\n",
    "merge_examples_df = pd.DataFrame(merge_examples).sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d54c0e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 lemmatization merges:\n",
      "\n",
      "'act' ← 3 variants:\n",
      "  acting, act, acts\n",
      "\n",
      "'rock' ← 2 variants:\n",
      "  rock, rocks\n",
      "\n",
      "'mystery' ← 2 variants:\n",
      "  mystery, mysteries\n",
      "\n",
      "'parent' ← 2 variants:\n",
      "  parents, parent\n",
      "\n",
      "'pediatric' ← 2 variants:\n",
      "  pediatrics, pediatric\n",
      "\n",
      "'physician' ← 2 variants:\n",
      "  physicians, physician\n",
      "\n",
      "'pictorial' ← 2 variants:\n",
      "  pictorials, pictorial\n",
      "\n",
      "'presentation' ← 2 variants:\n",
      "  presentations, presentation\n",
      "\n",
      "'record' ← 2 variants:\n",
      "  recording, records\n",
      "\n",
      "'review' ← 2 variants:\n",
      "  reviews, review\n",
      "\n",
      "'revolutionary period' ← 2 variants:\n",
      "  revolutionary period, revolutionary periods\n",
      "\n",
      "'security' ← 2 variants:\n",
      "  security, securities\n",
      "\n",
      "'meditation' ← 2 variants:\n",
      "  meditations, meditation\n",
      "\n",
      "'sound' ← 2 variants:\n",
      "  sounds, sound\n",
      "\n",
      "'speech' ← 2 variants:\n",
      "  speech, speeches\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 15 lemmatization merges:\")\n",
    "for idx, row in merge_examples_df.head(15).iterrows():\n",
    "    print(f\"\\n'{row['lemmatized']}' ← {row['count']} variants:\")\n",
    "    print(f\"  {', '.join(row['originals'][:5])}\", end='')\n",
    "    if row['count'] > 5:\n",
    "        print(f\" ... (+{row['count']-5} more)\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "525e3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update category frequency with lemmatized versions\n",
    "category_frequency = df_exploded['Category_Lemmatized'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "981e1824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 20 most frequent lemmatized categories:\n",
      "Category_Lemmatized\n",
      "fiction                17721\n",
      "juvenile fiction        6437\n",
      "religion                3697\n",
      "romance                 3590\n",
      "cook                    2927\n",
      "history                 2733\n",
      "juvenile nonfiction     2665\n",
      "economic                2627\n",
      "business                2601\n",
      "mystery                 2525\n",
      "historical              2356\n",
      "thriller                2299\n",
      "biography               2290\n",
      "autobiography           2290\n",
      "detective               2273\n",
      "health                  1997\n",
      "family                  1859\n",
      "political science       1850\n",
      "reference               1765\n",
      "literary                1764\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\nTop 20 most frequent lemmatized categories:\")\n",
    "print(category_frequency.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e98f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique categories: 2083\n"
     ]
    }
   ],
   "source": [
    "# Get all unique categories for next steps\n",
    "all_categories = df_exploded['Category_Lemmatized'].unique().tolist()\n",
    "print(f\"\\nTotal unique categories: {len(all_categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25979a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Found 485 book-category duplicates after lemmatization\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining inconsistencies\n",
    "duplicates = df_exploded.groupby(['book_id', 'Category_Lemmatized']).size()\n",
    "duplicates = duplicates[duplicates > 1]\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"\\nWarning: Found {len(duplicates)} book-category duplicates after lemmatization\")\n",
    "else:\n",
    "    print(\"\\n✓ No duplicates found - lemmatization successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1b183b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deduplication: 170,599 rows\n",
      "After deduplication: 170,114 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate book-category pairs after lemmatization\n",
    "print(f\"Before deduplication: {len(df_exploded):,} rows\")\n",
    "\n",
    "df_exploded = df_exploded.drop_duplicates(subset=['book_id', 'Category_Lemmatized']).reset_index(drop=True)\n",
    "\n",
    "print(f\"After deduplication: {len(df_exploded):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0045e4",
   "metadata": {},
   "source": [
    "## Hypernym-Based Label Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed38709",
   "metadata": {},
   "source": [
    "If we want to group different sports game into the general term 'sports', and we do not have the word 'sports' in our corpus, we need to use a hypernym dictionary for further merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3543e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out categories with very low frequency (e.g., less than 4 books)\n",
    "min_frequency = 4\n",
    "category_counts = df_exploded['Category_Lemmatized'].value_counts()\n",
    "filtered_categories = category_counts[category_counts >= min_frequency].index.tolist()\n",
    "df_exploded = df_exploded[df_exploded['Category_Lemmatized'].isin(filtered_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6faf7441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering low-frequency categories: 169,023 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"After filtering low-frequency categories: {len(df_exploded):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0df7a241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poetry',\n",
       " 'biography',\n",
       " 'autobiography',\n",
       " 'health',\n",
       " 'fitness',\n",
       " 'diet',\n",
       " 'nutrition',\n",
       " 'technology',\n",
       " 'engineering',\n",
       " 'military science']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show current list of categories\n",
    "all_categories = df_exploded['Category_Lemmatized'].unique().tolist()\n",
    "all_categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f687c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protected categories: 35\n"
     ]
    }
   ],
   "source": [
    "# Define protected categories that should not be merged based on the current list\n",
    "PROTECTED_CATEGORIES = {\n",
    "    # Fiction types\n",
    "    'fiction', 'science fiction', 'fantasy', 'mystery and detective', 'romance', \n",
    "    'thriller', 'suspense', 'historical', 'horror', 'western', 'literary',\n",
    "    # Age-specific\n",
    "    'juvenile fiction', 'juvenile nonfiction', 'young adult fiction', \n",
    "    'young adult nonfiction', 'adult',\n",
    "    # Major nonfiction genres\n",
    "    'biography and autobiography', 'history', 'religion', 'philosophy',\n",
    "    'science', 'reference', 'education', 'business and economic',\n",
    "    # Art forms\n",
    "    'poetry', 'drama', 'essay', 'short story',\n",
    "    # Core subject areas\n",
    "    'political science', 'social science', 'psychology', 'medical',\n",
    "    'technology and engineering', 'mathematics', 'computer',\n",
    "}\n",
    "print(f\"Protected categories: {len(PROTECTED_CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97284958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meaningful_hypernym(word, max_depth=4):\n",
    "    \"\"\"\n",
    "    Get domain-meaningful hypernym using smarter WordNet traversal.\n",
    "    \"\"\"\n",
    "    # Manual domain-specific mappings (override WordNet) - CHECK FIRST\n",
    "    MANUAL_HYPERNYMS = {\n",
    "        # Sports (explicit mapping to avoid WordNet inconsistencies)\n",
    "        'baseball': 'sport',\n",
    "        'basketball': 'sport',\n",
    "        'football': 'sport',\n",
    "        'golf': 'sport',\n",
    "        'soccer': 'sport',\n",
    "        'tennis': 'sport',\n",
    "        'hockey': 'sport',\n",
    "        'volleyball': 'sport',\n",
    "        'boxing': 'sport',\n",
    "        'wrestling': 'sport',\n",
    "        'swimming': 'sport',\n",
    "        'cycling': 'sport',\n",
    "        'running': 'sport',\n",
    "        'fishing': 'sport',\n",
    "        'sport and recreation': 'sport',\n",
    "        \n",
    "        # Food/Cooking\n",
    "        'cooking': 'food',\n",
    "        'cook': 'food',\n",
    "        'pasta': 'food',\n",
    "        'dessert': 'food',\n",
    "        'beverage': 'food',\n",
    "        'wine': 'food',\n",
    "        'bread': 'food',\n",
    "        'soup': 'food',\n",
    "        'salad': 'food',\n",
    "        \n",
    "        # Religion\n",
    "        'christian': 'religion',\n",
    "        'religious': 'religion',\n",
    "        'christianity': 'religion',\n",
    "        \n",
    "        # Fiction/Mystery - map to fiction parent\n",
    "        'mystery': 'fiction',\n",
    "        'thriller': 'fiction',\n",
    "        'contemporary': 'fiction',\n",
    "        \n",
    "        # Social sciences\n",
    "        'historical': 'history',\n",
    "        'political': 'political science',\n",
    "        'psychology': 'social science',\n",
    "        'sociology': 'social science',\n",
    "    }\n",
    "    \n",
    "    # Check manual mapping first\n",
    "    if word in MANUAL_HYPERNYMS:\n",
    "        return MANUAL_HYPERNYMS[word]\n",
    "    \n",
    "    # Domain-meaningful terms (accept these as hypernyms) - STOP here\n",
    "    MEANINGFUL_TERMS = {\n",
    "        # Core categories - STOP at these, don't go higher\n",
    "        'fiction', 'nonfiction',  # STOP here for literature\n",
    "        'sport', 'athletics', 'game', 'athletic game', 'field game', \n",
    "        'court game', 'ball game', 'outdoor sport', 'contact sport',\n",
    "        'food', 'dish', 'nutriment', 'course', 'ingredient', 'produce',\n",
    "        'meal', 'foodstuff', 'beverage', 'drink', 'nourishment',\n",
    "        'science', 'natural science', 'social science', 'applied science',\n",
    "        'art', 'fine art', 'music', 'dance', 'performing art', 'visual art',\n",
    "        'craft', 'hobby', 'health', 'medicine', 'medical specialty',\n",
    "        'education', 'business', 'religion', 'philosophy', 'travel',\n",
    "        'animal', 'plant', 'nature', 'technology', 'computer',\n",
    "        'drama', 'comedy', 'activity', 'diversion', 'entertainment',\n",
    "        'history', 'political science',\n",
    "        # Lower-level acceptable terms\n",
    "        'life science', 'physical science', 'therapy',\n",
    "        'instruction', 'learning', 'discipline', 'commerce', 'trade',\n",
    "        'organism', 'living thing', 'flora', 'vehicle', 'building'\n",
    "    }\n",
    "    \n",
    "    # Reject over-general terms\n",
    "    REJECT_TERMS = {\n",
    "        'writing', 'written communication', 'literary composition',  # Too general for fiction\n",
    "        'abstraction', 'entity', 'physical entity', 'object', 'whole', \n",
    "        'thing', 'matter', 'substance', 'relation', 'communication',\n",
    "        'social relation', 'attribute', 'psychological feature', \n",
    "        'cognition', 'content', 'message', 'state', 'possession'\n",
    "    }\n",
    "    \n",
    "    # Check if it's a compound in WordNet\n",
    "    compound_synsets = wn.synsets(word.replace(' ', '_'), pos='n')\n",
    "    if compound_synsets:\n",
    "        synsets = compound_synsets\n",
    "    else:\n",
    "        # Try first word (head noun for multi-word)\n",
    "        first_word = word.split()[0]\n",
    "        synsets = wn.synsets(first_word, pos='n')\n",
    "    \n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    # Try multiple senses\n",
    "    for synset in synsets[:3]:\n",
    "        current = synset\n",
    "        \n",
    "        # Traverse up to max_depth\n",
    "        for depth in range(max_depth + 1):\n",
    "            hypernyms = current.hypernyms()\n",
    "            if not hypernyms:\n",
    "                break\n",
    "            \n",
    "            hyper = hypernyms[0]\n",
    "            hyper_name = hyper.name().split('.')[0].replace('_', ' ')\n",
    "            \n",
    "            # Check if should reject first\n",
    "            if hyper_name in REJECT_TERMS:\n",
    "                break\n",
    "            \n",
    "            # Check if meaningful - STOP here\n",
    "            if hyper_name in MEANINGFUL_TERMS:\n",
    "                return hyper_name\n",
    "            \n",
    "            current = hyper\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea3e0b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypernym extraction examples:\n",
      "  'baseball' → 'sport'\n",
      "  'basketball' → 'sport'\n",
      "  'cooking' → 'food'\n",
      "  'pasta' → 'food'\n",
      "  'fiction' → 'None'\n",
      "  'mystery' → 'fiction'\n"
     ]
    }
   ],
   "source": [
    "# Test on sample\n",
    "test_words = ['baseball', 'basketball', 'cooking', 'pasta', 'fiction', 'mystery']\n",
    "print(\"\\nHypernym extraction examples:\")\n",
    "for word in test_words:\n",
    "    hyper = get_meaningful_hypernym(word)\n",
    "    print(f\"  '{word}' → '{hyper}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da286087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypernym mapping statistics:\n",
      "  Total categories processed: 1449\n",
      "  Protected (kept as-is): 31\n",
      "  Mapped to hypernyms: 395\n",
      "  Unmapped (kept original): 1023\n",
      "\n",
      "Unique hypernym labels: 1084\n"
     ]
    }
   ],
   "source": [
    "# Build complete hypernym mapping for all frequent categories\n",
    "hypernym_mapping = {}\n",
    "unmapped_categories = []\n",
    "protected_count = 0\n",
    "manual_count = 0\n",
    "wordnet_count = 0\n",
    "\n",
    "for cat in filtered_categories:\n",
    "    # Skip protected categories\n",
    "    if cat in PROTECTED_CATEGORIES or cat.lower() in PROTECTED_CATEGORIES:\n",
    "        hypernym_mapping[cat] = cat\n",
    "        protected_count += 1\n",
    "        continue\n",
    "    \n",
    "    # All categories are now atomic (no compounds with 'and')\n",
    "    hyper = get_meaningful_hypernym(cat)\n",
    "    \n",
    "    if hyper:\n",
    "        hypernym_mapping[cat] = hyper\n",
    "        # Approximate count (manual mappings handled inside function)\n",
    "        wordnet_count += 1\n",
    "    else:\n",
    "        hypernym_mapping[cat] = cat  # Keep as-is\n",
    "        unmapped_categories.append(cat)\n",
    "\n",
    "print(f\"\\nHypernym mapping statistics:\")\n",
    "print(f\"  Total categories processed: {len(hypernym_mapping)}\")\n",
    "print(f\"  Protected (kept as-is): {protected_count}\")\n",
    "print(f\"  Mapped to hypernyms: {len([k for k, v in hypernym_mapping.items() if k != v])}\")\n",
    "print(f\"  Unmapped (kept original): {len(unmapped_categories)}\")\n",
    "print(f\"\\nUnique hypernym labels: {len(set(hypernym_mapping.values()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77350e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 hypernym groupings (by number of categories merged):\n",
      "======================================================================\n",
      "\n",
      "'activity' ← 91 categories:\n",
      "  recreation, craft, art, adventure, technology, engineering, method, photography, games, music ... (+81 more)\n",
      "\n",
      "'commerce' ← 20 categories:\n",
      "  business, investment, marketing, business communication, industry, business development, sale, sell, finance, desktop publishing ... (+10 more)\n",
      "\n",
      "'discipline' ← 19 categories:\n",
      "  discipline, literary criticism, english, trivium, military science, computer science, genealogy, telecommunication, literature, information technology ... (+9 more)\n",
      "\n",
      "'medicine' ← 17 categories:\n",
      "  specific ingredient, psychotherapy, psychiatry, gerontology, gynecology, oncology, clinical psychology, pharmacology, complementary medicine, endocrinology ... (+7 more)\n",
      "\n",
      "'food' ← 16 categories:\n",
      "  cook, nutrition, diet, beverage, wine, food, dessert, seafood, pasta, soup ... (+6 more)\n",
      "\n",
      "'religion' ← 14 categories:\n",
      "  religion, religious, christian, christianity, judaism, islam, buddhism, christian church, yoga, cult ... (+4 more)\n",
      "\n",
      "'animal' ← 14 categories:\n",
      "  pet, reptile, insect, spider, bird, fish, mammal, farm animal, poultry, adult child ... (+4 more)\n",
      "\n",
      "'sport' ← 14 categories:\n",
      "  baseball, golf, football, basketball, cycling, soccer, fishing, horse racing, tennis, swim ... (+4 more)\n",
      "\n",
      "'science' ← 13 categories:\n",
      "  science, psychopathology, grammar, algebra, social psychology, calculus, cognitive psychology, geometry, arithmetic, information science ... (+3 more)\n",
      "\n",
      "'building' ← 12 categories:\n",
      "  house, school, architecture, grill, theater, lodge, house plan, restaurant, house plant, hostel ... (+2 more)\n",
      "\n",
      "'natural science' ← 12 categories:\n",
      "  life science, earth science, physics, astronomy, chemistry, geography, climatology, meteorology, biochemistry, geology ... (+2 more)\n",
      "\n",
      "'life science' ← 12 categories:\n",
      "  physiology, biology, environmental science, anatomy, ecology, zoology, neuroscience, molecular biology, neurology, microbiology ... (+2 more)\n",
      "\n",
      "'music' ← 11 categories:\n",
      "  chapter book, theme, genre, quotation, classical, spiritual growth, spiritual, opera, pastoral resource, jazz ... (+1 more)\n",
      "\n",
      "'plant' ← 11 categories:\n",
      "  flower, herb, perennial, flower arrange, annual, tree, rose, bluegrass, ornamental plant, shrub ... (+1 more)\n",
      "\n",
      "'fiction' ← 9 categories:\n",
      "  fiction, mystery, contemporary, detective story, sagas, legend, myth, fable, story in verse\n",
      "\n",
      "\n",
      "Sample unmapped categories (kept as original):\n",
      "  - economic\n",
      "  - detective\n",
      "  - health\n",
      "  - family\n",
      "  - fitness\n",
      "  - united states\n",
      "  - relationship\n",
      "  - travel\n",
      "  - self help\n",
      "  - humor\n",
      "  - reader\n",
      "  - social theme\n",
      "  - home\n",
      "  - nature\n",
      "  - military\n",
      "  - parenting\n",
      "  - woman\n",
      "  - christian life\n",
      "  - garden\n",
      "  - inspirational\n"
     ]
    }
   ],
   "source": [
    "# Group categories by their hypernym\n",
    "hypernym_groups = defaultdict(list)\n",
    "for orig, hyper in hypernym_mapping.items():\n",
    "    hypernym_groups[hyper].append(orig)\n",
    "\n",
    "# Sort by number of categories merged\n",
    "sorted_groups = sorted(hypernym_groups.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nTop 15 hypernym groupings (by number of categories merged):\\n\" + \"=\"*70)\n",
    "for hyper, originals in sorted_groups[:15]:\n",
    "    print(f\"\\n'{hyper}' ← {len(originals)} categories:\")\n",
    "    if len(originals) == 1:\n",
    "        print(f\"  (kept as-is: {originals[0]})\")\n",
    "    else:\n",
    "        print(f\"  {', '.join(originals[:10])}\", end='')\n",
    "        if len(originals) > 10:\n",
    "            print(f\" ... (+{len(originals)-10} more)\")\n",
    "        else:\n",
    "            print()\n",
    "            \n",
    "# Show some unmapped categories for review\n",
    "if unmapped_categories:\n",
    "    print(f\"\\n\\nSample unmapped categories (kept as original):\")\n",
    "    for cat in unmapped_categories[:20]:\n",
    "        print(f\"  - {cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d797aebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before hypernym deduplication: 169,023 rows\n",
      "After hypernym deduplication: 158,160 rows\n",
      "\n",
      "✓ Final dataset:\n",
      "  Books: 65,294\n",
      "  Hypernym labels: 1,084\n",
      "  Avg labels per book: 2.42\n"
     ]
    }
   ],
   "source": [
    "# Apply mapping\n",
    "df_exploded['Hypernym_Label'] = df_exploded['Category_Lemmatized'].map(hypernym_mapping)\n",
    "\n",
    "print(f\"Before hypernym deduplication: {len(df_exploded):,} rows\")\n",
    "\n",
    "# Remove duplicates (same book + same hypernym from different originals)\n",
    "df_exploded = df_exploded.drop_duplicates(\n",
    "    subset=['book_id', 'Hypernym_Label']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"After hypernym deduplication: {len(df_exploded):,} rows\")\n",
    "\n",
    "# Final stats\n",
    "print(f\"\\n✓ Final dataset:\")\n",
    "print(f\"  Books: {df_exploded['book_id'].nunique():,}\")\n",
    "print(f\"  Hypernym labels: {df_exploded['Hypernym_Label'].nunique():,}\")\n",
    "print(f\"  Avg labels per book: {len(df_exploded) / df_exploded['book_id'].nunique():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "578768e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Hypernym_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Collects poems written by the eleven-year-old ...</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Russian author offers an affectionate chro...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A humor classic, this tongue-in-cheek diet pla...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A humor classic, this tongue-in-cheek diet pla...</td>\n",
       "      <td>fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A humor classic, this tongue-in-cheek diet pla...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deadly germs sprayed in shopping malls, bomb-l...</td>\n",
       "      <td>activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deadly germs sprayed in shopping malls, bomb-l...</td>\n",
       "      <td>discipline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"The Bible and the social and moral consequenc...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"The Bible and the social and moral consequenc...</td>\n",
       "      <td>biblical biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A New York Times Notable Book of the YearThis ...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description      Hypernym_Label\n",
       "0  Collects poems written by the eleven-year-old ...              poetry\n",
       "1  The Russian author offers an affectionate chro...             history\n",
       "2  A humor classic, this tongue-in-cheek diet pla...              health\n",
       "3  A humor classic, this tongue-in-cheek diet pla...             fitness\n",
       "4  A humor classic, this tongue-in-cheek diet pla...                food\n",
       "5  Deadly germs sprayed in shopping malls, bomb-l...            activity\n",
       "6  Deadly germs sprayed in shopping malls, bomb-l...          discipline\n",
       "7  \"The Bible and the social and moral consequenc...            religion\n",
       "8  \"The Bible and the social and moral consequenc...  biblical biography\n",
       "9  A New York Times Notable Book of the YearThis ...             history"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded[['Description','Hypernym_Label']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db6eec",
   "metadata": {},
   "source": [
    "# Description Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f9a9c",
   "metadata": {},
   "source": [
    "Since we are using pre-trained Sentence-BERT models (which were trained on natural text), we should do minimal preprocessing to preserve semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_description(text):\n",
    "    \"\"\"\n",
    "    Basic cleaning for description text before SBERT embedding.\n",
    "    Preserves semantic content while removing noise.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Decode HTML entities (e.g., &amp; → &, &quot; → \")\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Fix common encoding issues\n",
    "    text = text.replace('â€™', \"'\")\n",
    "    text = text.replace('â€œ', '\"')\n",
    "    text = text.replace('â€', '\"')\n",
    "    text = text.replace('â€\"', '—')\n",
    "    \n",
    "    # Collapse multiple whitespaces/newlines to single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bd3e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning descriptions...\n",
      "Total books: 65296\n",
      "Empty descriptions after cleaning: 0\n",
      "Avg description length (words): 122.8\n",
      "\n",
      "\n",
      "Sample description cleaning:\n",
      "======================================================================\n",
      "\n",
      "Original (176 chars):\n",
      "  Collects poems written by the eleven-year-old muscular dystrophy patient, sharing his feelings and thoughts about his life, the deaths of his siblings...\n",
      "\n",
      "Cleaned (176 chars):\n",
      "  Collects poems written by the eleven-year-old muscular dystrophy patient, sharing his feelings and thoughts about his life, the deaths of his siblings...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original (197 chars):\n",
      "  The Russian author offers an affectionate chronicle of life in the United States, with discussions of such topics as the European charm of Washington,...\n",
      "\n",
      "Cleaned (197 chars):\n",
      "  The Russian author offers an affectionate chronicle of life in the United States, with discussions of such topics as the European charm of Washington,...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original (437 chars):\n",
      "  A humor classic, this tongue-in-cheek diet plan bases calorie counts on sexual activity and charts the exact number of calories burned.\"You and your f...\n",
      "\n",
      "Cleaned (437 chars):\n",
      "  A humor classic, this tongue-in-cheek diet plan bases calorie counts on sexual activity and charts the exact number of calories burned.\"You and your f...\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "# Apply cleaning to descriptions\n",
    "print(\"Cleaning descriptions...\")\n",
    "df['Description_Cleaned'] = df['Description'].apply(clean_description)\n",
    "\n",
    "# Check results\n",
    "print(f\"Total books: {len(df)}\")\n",
    "print(f\"Empty descriptions after cleaning: {(df['Description_Cleaned'] == '').sum()}\")\n",
    "print(f\"Avg description length (words): {df['Description_Cleaned'].str.split().str.len().mean():.1f}\")\n",
    "\n",
    "# Show sample before/after\n",
    "print(\"\\n\\nSample description cleaning:\\n\" + \"=\"*70)\n",
    "for idx in df[df['Description_Cleaned'] != ''].head(3).index:\n",
    "    print(f\"\\nOriginal ({len(df.loc[idx, 'Description'])} chars):\")\n",
    "    print(f\"  {df.loc[idx, 'Description'][:150]}...\")\n",
    "    print(f\"\\nCleaned ({len(df.loc[idx, 'Description_Cleaned'])} chars):\")\n",
    "    print(f\"  {df.loc[idx, 'Description_Cleaned'][:150]}...\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72251594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Price Starting With ($)</th>\n",
       "      <th>Publish Date (Month)</th>\n",
       "      <th>Publish Date (Year)</th>\n",
       "      <th>book_id</th>\n",
       "      <th>Category_Cleaned</th>\n",
       "      <th>Description_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Journey Through Heartsongs</td>\n",
       "      <td>By Stepanek, Mattie J. T.</td>\n",
       "      <td>Collects poems written by the eleven-year-old muscular dystrophy patient, sharing his feelings and thoughts about his life, the deaths of his siblings, nature, faith, and hope.</td>\n",
       "      <td>Poetry , General</td>\n",
       "      <td>VSP Books</td>\n",
       "      <td>19.96</td>\n",
       "      <td>September</td>\n",
       "      <td>2001</td>\n",
       "      <td>7</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Collects poems written by the eleven-year-old muscular dystrophy patient, sharing his feelings and thoughts about his life, the deaths of his siblings, nature, faith, and hope.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In Search of Melancholy Baby</td>\n",
       "      <td>By Aksyonov, Vassily, Heim, Michael Henry, and Bouis, Antonina W.</td>\n",
       "      <td>The Russian author offers an affectionate chronicle of life in the United States, with discussions of such topics as the European charm of Washington, D.C., and the American immigration bureaucracy</td>\n",
       "      <td>Biography &amp; Autobiography , General</td>\n",
       "      <td>Random House</td>\n",
       "      <td>4.99</td>\n",
       "      <td>June</td>\n",
       "      <td>1987</td>\n",
       "      <td>8</td>\n",
       "      <td>biography, autobiography</td>\n",
       "      <td>The Russian author offers an affectionate chronicle of life in the United States, with discussions of such topics as the European charm of Washington, D.C., and the American immigration bureaucracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Dieter's Guide to Weight Loss During Sex</td>\n",
       "      <td>By Smith, Richard</td>\n",
       "      <td>A humor classic, this tongue-in-cheek diet plan bases calorie counts on sexual activity and charts the exact number of calories burned.\"You and your friends can read this book aloud while sitting ...</td>\n",
       "      <td>Health &amp; Fitness , Diet &amp; Nutrition , Diets</td>\n",
       "      <td>Workman Publishing Company</td>\n",
       "      <td>4.99</td>\n",
       "      <td>January</td>\n",
       "      <td>1978</td>\n",
       "      <td>10</td>\n",
       "      <td>health, fitness, diet, nutrition, diets</td>\n",
       "      <td>A humor classic, this tongue-in-cheek diet plan bases calorie counts on sexual activity and charts the exact number of calories burned.\"You and your friends can read this book aloud while sitting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Germs : Biological Weapons and America's Secret War</td>\n",
       "      <td>By Miller, Judith, Engelberg, Stephen, and Broad, William J.</td>\n",
       "      <td>Deadly germs sprayed in shopping malls, bomb-lets spewing anthrax spores over battlefields, tiny vials of plague scattered in Times Square -- these are the poor man's hydrogen bombs, hideous weapo...</td>\n",
       "      <td>Technology &amp; Engineering , Military Science</td>\n",
       "      <td>Simon &amp; Schuster</td>\n",
       "      <td>4.99</td>\n",
       "      <td>October</td>\n",
       "      <td>2001</td>\n",
       "      <td>11</td>\n",
       "      <td>technology, engineering, military science</td>\n",
       "      <td>Deadly germs sprayed in shopping malls, bomb-lets spewing anthrax spores over battlefields, tiny vials of plague scattered in Times Square -- these are the poor man's hydrogen bombs, hideous weapo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Good Book: Reading the Bible with Mind and Heart</td>\n",
       "      <td>By Gomes, Peter J.</td>\n",
       "      <td>\"The Bible and the social and moral consequences that derive from its interpretation are all too important to be left in the hands of the pious or the experts, and too significant to be ignored an...</td>\n",
       "      <td>Religion , Biblical Biography , General</td>\n",
       "      <td>Harper Perennial</td>\n",
       "      <td>5.29</td>\n",
       "      <td>May</td>\n",
       "      <td>1998</td>\n",
       "      <td>13</td>\n",
       "      <td>religion, biblical biography</td>\n",
       "      <td>\"The Bible and the social and moral consequences that derive from its interpretation are all too important to be left in the hands of the pious or the experts, and too significant to be ignored an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>All over but the Shoutin'</td>\n",
       "      <td>By Bragg, Rick</td>\n",
       "      <td>A New York Times Notable Book of the YearThis haunting, harrowing, gloriously moving recollection of a life on the American margin is the story of Rick Bragg, who grew up dirt-poor in northeastern...</td>\n",
       "      <td>Biography &amp; Autobiography , Personal Memoirs</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>4.89</td>\n",
       "      <td>September</td>\n",
       "      <td>1998</td>\n",
       "      <td>14</td>\n",
       "      <td>biography, autobiography, personal memoirs</td>\n",
       "      <td>A New York Times Notable Book of the YearThis haunting, harrowing, gloriously moving recollection of a life on the American margin is the story of Rick Bragg, who grew up dirt-poor in northeastern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hill Rat: Blowing the Lid Off Congress</td>\n",
       "      <td>By Jackley, John L.</td>\n",
       "      <td>As a top aide to powerful Texas Congressman Ronald Coleman, John Jackley watched the nation's highest elected officials lie, manipulate media images, and spend millions of taxpayer dollars to stay...</td>\n",
       "      <td>Political Science , General</td>\n",
       "      <td>Regnery Publishing, Inc.</td>\n",
       "      <td>4.99</td>\n",
       "      <td>April</td>\n",
       "      <td>1992</td>\n",
       "      <td>19</td>\n",
       "      <td>political science</td>\n",
       "      <td>As a top aide to powerful Texas Congressman Ronald Coleman, John Jackley watched the nation's highest elected officials lie, manipulate media images, and spend millions of taxpayer dollars to stay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Personality of the Cat</td>\n",
       "      <td>By Aymar, Brandt (EDT)</td>\n",
       "      <td>A delightful collection of reflections on the feline contains selections from famous artists, writers, poets, and essayists, such as Pablo Picasso, Rudyard Kipling, T. S. Eliot, and others.</td>\n",
       "      <td>Pets , Cats , General</td>\n",
       "      <td>Bonanza Books</td>\n",
       "      <td>5.41</td>\n",
       "      <td>January</td>\n",
       "      <td>1978</td>\n",
       "      <td>21</td>\n",
       "      <td>pets, cats</td>\n",
       "      <td>A delightful collection of reflections on the feline contains selections from famous artists, writers, poets, and essayists, such as Pablo Picasso, Rudyard Kipling, T. S. Eliot, and others.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Betrayal : How the Clinton Administration Undermined American Security</td>\n",
       "      <td>By Gertz, Bill</td>\n",
       "      <td>Gertz, writer for the Moonie-influenced  Washington Times  newspaper and a reporter  with the highest right-wing credentials (including praise from Rush Limbaugh and G. Gordon Liddy on the jacket)...</td>\n",
       "      <td>Political Science , General</td>\n",
       "      <td>Regnery Pub</td>\n",
       "      <td>4.99</td>\n",
       "      <td>May</td>\n",
       "      <td>1999</td>\n",
       "      <td>26</td>\n",
       "      <td>political science</td>\n",
       "      <td>Gertz, writer for the Moonie-influenced Washington Times newspaper and a reporter with the highest right-wing credentials (including praise from Rush Limbaugh and G. Gordon Liddy on the jacket) us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Shadow Song</td>\n",
       "      <td>By Kay, Terry</td>\n",
       "      <td>It was a wonderful summer, a great memory, the kind of love everybody ought to have. It changed my life - or almost did - and I think about it more than I should, but that was a long time ago.Amid...</td>\n",
       "      <td>Fiction , General</td>\n",
       "      <td>Atria Books</td>\n",
       "      <td>4.99</td>\n",
       "      <td>October</td>\n",
       "      <td>1994</td>\n",
       "      <td>27</td>\n",
       "      <td>fiction</td>\n",
       "      <td>It was a wonderful summer, a great memory, the kind of love everybody ought to have. It changed my life - or almost did - and I think about it more than I should, but that was a long time ago.Amid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Title  \\\n",
       "7                                               Journey Through Heartsongs   \n",
       "8                                             In Search of Melancholy Baby   \n",
       "10                            The Dieter's Guide to Weight Loss During Sex   \n",
       "11                     Germs : Biological Weapons and America's Secret War   \n",
       "13                    The Good Book: Reading the Bible with Mind and Heart   \n",
       "14                                               All over but the Shoutin'   \n",
       "19                                  Hill Rat: Blowing the Lid Off Congress   \n",
       "21                                                  Personality of the Cat   \n",
       "26  Betrayal : How the Clinton Administration Undermined American Security   \n",
       "27                                                             Shadow Song   \n",
       "\n",
       "                                                              Authors  \\\n",
       "7                                           By Stepanek, Mattie J. T.   \n",
       "8   By Aksyonov, Vassily, Heim, Michael Henry, and Bouis, Antonina W.   \n",
       "10                                                  By Smith, Richard   \n",
       "11       By Miller, Judith, Engelberg, Stephen, and Broad, William J.   \n",
       "13                                                 By Gomes, Peter J.   \n",
       "14                                                     By Bragg, Rick   \n",
       "19                                                By Jackley, John L.   \n",
       "21                                             By Aymar, Brandt (EDT)   \n",
       "26                                                     By Gertz, Bill   \n",
       "27                                                      By Kay, Terry   \n",
       "\n",
       "                                                                                                                                                                                                Description  \\\n",
       "7                          Collects poems written by the eleven-year-old muscular dystrophy patient, sharing his feelings and thoughts about his life, the deaths of his siblings, nature, faith, and hope.   \n",
       "8     The Russian author offers an affectionate chronicle of life in the United States, with discussions of such topics as the European charm of Washington, D.C., and the American immigration bureaucracy   \n",
       "10  A humor classic, this tongue-in-cheek diet plan bases calorie counts on sexual activity and charts the exact number of calories burned.\"You and your friends can read this book aloud while sitting ...   \n",
       "11  Deadly germs sprayed in shopping malls, bomb-lets spewing anthrax spores over battlefields, tiny vials of plague scattered in Times Square -- these are the poor man's hydrogen bombs, hideous weapo...   \n",
       "13  \"The Bible and the social and moral consequences that derive from its interpretation are all too important to be left in the hands of the pious or the experts, and too significant to be ignored an...   \n",
       "14  A New York Times Notable Book of the YearThis haunting, harrowing, gloriously moving recollection of a life on the American margin is the story of Rick Bragg, who grew up dirt-poor in northeastern...   \n",
       "19  As a top aide to powerful Texas Congressman Ronald Coleman, John Jackley watched the nation's highest elected officials lie, manipulate media images, and spend millions of taxpayer dollars to stay...   \n",
       "21            A delightful collection of reflections on the feline contains selections from famous artists, writers, poets, and essayists, such as Pablo Picasso, Rudyard Kipling, T. S. Eliot, and others.   \n",
       "26  Gertz, writer for the Moonie-influenced  Washington Times  newspaper and a reporter  with the highest right-wing credentials (including praise from Rush Limbaugh and G. Gordon Liddy on the jacket)...   \n",
       "27  It was a wonderful summer, a great memory, the kind of love everybody ought to have. It changed my life - or almost did - and I think about it more than I should, but that was a long time ago.Amid...   \n",
       "\n",
       "                                         Category                   Publisher  \\\n",
       "7                                Poetry , General                   VSP Books   \n",
       "8             Biography & Autobiography , General                Random House   \n",
       "10    Health & Fitness , Diet & Nutrition , Diets  Workman Publishing Company   \n",
       "11    Technology & Engineering , Military Science            Simon & Schuster   \n",
       "13        Religion , Biblical Biography , General            Harper Perennial   \n",
       "14   Biography & Autobiography , Personal Memoirs                     Vintage   \n",
       "19                    Political Science , General    Regnery Publishing, Inc.   \n",
       "21                          Pets , Cats , General               Bonanza Books   \n",
       "26                    Political Science , General                 Regnery Pub   \n",
       "27                              Fiction , General                 Atria Books   \n",
       "\n",
       "    Price Starting With ($) Publish Date (Month)  Publish Date (Year)  \\\n",
       "7                     19.96            September                 2001   \n",
       "8                      4.99                 June                 1987   \n",
       "10                     4.99              January                 1978   \n",
       "11                     4.99              October                 2001   \n",
       "13                     5.29                  May                 1998   \n",
       "14                     4.89            September                 1998   \n",
       "19                     4.99                April                 1992   \n",
       "21                     5.41              January                 1978   \n",
       "26                     4.99                  May                 1999   \n",
       "27                     4.99              October                 1994   \n",
       "\n",
       "    book_id                            Category_Cleaned  \\\n",
       "7         7                                      poetry   \n",
       "8         8                    biography, autobiography   \n",
       "10       10     health, fitness, diet, nutrition, diets   \n",
       "11       11   technology, engineering, military science   \n",
       "13       13                religion, biblical biography   \n",
       "14       14  biography, autobiography, personal memoirs   \n",
       "19       19                           political science   \n",
       "21       21                                  pets, cats   \n",
       "26       26                           political science   \n",
       "27       27                                     fiction   \n",
       "\n",
       "                                                                                                                                                                                        Description_Cleaned  \n",
       "7                          Collects poems written by the eleven-year-old muscular dystrophy patient, sharing his feelings and thoughts about his life, the deaths of his siblings, nature, faith, and hope.  \n",
       "8     The Russian author offers an affectionate chronicle of life in the United States, with discussions of such topics as the European charm of Washington, D.C., and the American immigration bureaucracy  \n",
       "10  A humor classic, this tongue-in-cheek diet plan bases calorie counts on sexual activity and charts the exact number of calories burned.\"You and your friends can read this book aloud while sitting ...  \n",
       "11  Deadly germs sprayed in shopping malls, bomb-lets spewing anthrax spores over battlefields, tiny vials of plague scattered in Times Square -- these are the poor man's hydrogen bombs, hideous weapo...  \n",
       "13  \"The Bible and the social and moral consequences that derive from its interpretation are all too important to be left in the hands of the pious or the experts, and too significant to be ignored an...  \n",
       "14  A New York Times Notable Book of the YearThis haunting, harrowing, gloriously moving recollection of a life on the American margin is the story of Rick Bragg, who grew up dirt-poor in northeastern...  \n",
       "19  As a top aide to powerful Texas Congressman Ronald Coleman, John Jackley watched the nation's highest elected officials lie, manipulate media images, and spend millions of taxpayer dollars to stay...  \n",
       "21            A delightful collection of reflections on the feline contains selections from famous artists, writers, poets, and essayists, such as Pablo Picasso, Rudyard Kipling, T. S. Eliot, and others.  \n",
       "26  Gertz, writer for the Moonie-influenced Washington Times newspaper and a reporter with the highest right-wing credentials (including praise from Rush Limbaugh and G. Gordon Liddy on the jacket) us...  \n",
       "27  It was a wonderful summer, a great memory, the kind of love everybody ought to have. It changed my life - or almost did - and I think about it more than I should, but that was a long time ago.Amid...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5400be",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63176b15",
   "metadata": {},
   "source": [
    "## Description Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea1590ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating description embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2041/2041 [03:19<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (65296, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model again for description embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for all books (one per book)\n",
    "print(\"Generating description embeddings...\")\n",
    "descriptions = df['Description_Cleaned'].tolist()\n",
    "description_embeddings = model.encode(descriptions, \n",
    "                                     show_progress_bar=True,\n",
    "                                     batch_size=32)\n",
    "\n",
    "# Save embeddings\n",
    "np.save('description_embeddings.npy', description_embeddings)\n",
    "print(f\"Shape: {description_embeddings.shape}\")  # Should be (65296, 384)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1764293",
   "metadata": {},
   "source": [
    "## Label Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2f3ae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 34/34 [00:01<00:00, 19.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1084, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique hypernym labels\n",
    "unique_labels = sorted(df_exploded['Hypernym_Label'].unique().tolist())\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "\n",
    "# Generate label embeddings\n",
    "label_embeddings = model.encode(unique_labels,\n",
    "                                show_progress_bar=True)\n",
    "\n",
    "# Save\n",
    "np.save('label_embeddings.npy', label_embeddings)\n",
    "np.save('label_to_idx.npy', label_to_idx)\n",
    "print(f\"Shape: {label_embeddings.shape}\")  # Should be (N_labels, 384)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e7256",
   "metadata": {},
   "source": [
    "## Create Multi-Label Target Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "563f96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target matrix shape: (65296, 1084)\n",
      "Avg labels per book: 2.42\n"
     ]
    }
   ],
   "source": [
    "# Create binary label matrix (books × labels)\n",
    "n_books = len(df)\n",
    "n_labels = len(unique_labels)\n",
    "\n",
    "y_multilabel = np.zeros((n_books, n_labels), dtype=np.float32)\n",
    "\n",
    "# Fill in labels for each book\n",
    "for position, (idx, row) in enumerate(df.iterrows()):\n",
    "    book_id = row['book_id']\n",
    "    # Get all hypernym labels for this book\n",
    "    book_labels = df_exploded[df_exploded['book_id'] == book_id]['Hypernym_Label'].tolist()\n",
    "    \n",
    "    # Set corresponding positions to 1\n",
    "    for label in book_labels:\n",
    "        label_idx = label_to_idx[label]\n",
    "        y_multilabel[position, label_idx] = 1.0\n",
    "\n",
    "# Save\n",
    "np.save('y_multilabel.npy', y_multilabel)\n",
    "print(f\"Target matrix shape: {y_multilabel.shape}\")\n",
    "print(f\"Avg labels per book: {y_multilabel.sum(axis=1).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef206a18",
   "metadata": {},
   "source": [
    "# Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114879b",
   "metadata": {},
   "source": [
    "The training use description_embeddings.npy as X (input features) and y_multilabel.npy as y (target labels) to train a neural network:\n",
    "\n",
    "Input layer: 384 units (description embedding)<br>\n",
    "Hidden layers: Dense layers with dropout/batch norm<br>\n",
    "Output layer: N units with sigmoid activation (one per label)<br>\n",
    "Loss: Binary Cross-Entropy<br>\n",
    "Evaluation: Multilabel metrics (precision, recall, F1 per label and micro/macro averages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
